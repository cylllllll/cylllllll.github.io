<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on cy&#39;s Blog 🍉</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on cy&#39;s Blog 🍉</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>Copyright © 2021, all rights reserved.</copyright>
    <lastBuildDate>Tue, 07 May 2019 10:00:00 +0000</lastBuildDate><atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>逻辑回归</title>
      <link>/post/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Tue, 07 May 2019 10:00:00 +0000</pubDate>
      
      <guid>/post/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</guid>
      <description>逻辑回归  从大的类别上来说，逻辑回归是一种有监督的统计学习方法，主要用于对样本进行分类。在线性回归模型中，输出一般是连续的，例如
 $$ y=f(x)=ax+b $$
 对于每一个输入的x，都有一个对应的y输出。模型的定义域和值域都可以是[-∞, +∞]。但是对于逻辑回归，输入可以是连续的[-∞, +∞]，但输出一般是离散的，即只有有限多个输出值。例如，其值域可以只有两个值{0, 1}，这两个值可以表示对样本的某种分类，高/低、患病/健康、阴性/阳性等，这就是最常见的二分类逻辑回归。因此，从整体上来说，通过逻辑回归模型，我们将在整个实数范围上的x映射到了有限个点上，这样就实现了对x的分类。因为每次拿过来一个x，经过逻辑回归分析，就可以将它归入某一类y中。
​	逻辑回归也被称为广义线性回归模型，它与线性回归模型的形式基本上相同，都具有 ax+b，其中a和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将ax+b作为因变量，即y = ax+b，而logistic回归则通过函数S将ax+b对应到一个隐状态p，p = S(ax+b)，然后根据p与1-p的大小决定因变量的值。这里的函数S就是Sigmoid函数
 $$ S(t)=\frac{1}{1+e−t} $$
 将t换成ax+b，可以得到逻辑回归模型的参数形式：
 $$
$$
​	逻辑回归返回的是概率。可以使用返回的概率（例如，用户点击此广告的概率为 0.000054），也可以将返回的概率转换成二元值（例如，这封电子邮件是垃圾邮件）。为了将逻辑回归值映射到二元类别，必须指定分类阈值。如果值高于该阈值，则表示“垃圾邮件”；如果值低于该阈值，则表示“非垃圾邮件”。通常来说分类阈值应始终为 0.5，但阈值取决于具体问题，因此须根据情况进行调整。
该模型将 100 个分为真 （正类别）或假（负类别）
   真正例（TP）     真实情况：假   机器学习模型预测结果：假   TP结果数：1       假正例（FP）     真实情况：真   机器学习模型预测结果：假   TP结果数：1       假负例（FN）     真实情况：假   机器学习模型预测结果：真   TP结果数：8       真负例（TN）     真实情况：真   机器学习模型预测结果：真   TP结果数：90     准确率 准确率是指我们的模型预测正确的结果所占的比例 $$ Accuracy=\frac{TP+TN}{TP+TN+FP+FN}=\frac{1+90}{1+90+1+8}=0.</description>
    </item>
    
  </channel>
</rss>
